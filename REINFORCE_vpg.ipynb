{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA+t0jnQlQH02y+KCmWdM1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benteaux/sketchbook/blob/main/REINFORCE_vpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "read later:\n",
        "1. https://arxiv.org/abs/1506.02438\n",
        "2. https://openai.com/research/vpt\n",
        "3. https://openai.com/research/openai-gym-beta"
      ],
      "metadata": {
        "id": "18Kgvekx3kvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box"
      ],
      "metadata": {
        "id": "iZrgm1Ds3kAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYk097TQpxtl"
      },
      "outputs": [],
      "source": [
        "# feedforward network\n",
        "def mlp(sizes, activation = nn.Tanh, output_activation = nn.Identity):\n",
        "  layers = []\n",
        "  for j in range(len(sizes) - 1):\n",
        "    act = activation if j < len(sizes) - 2 else output_activation\n",
        "    layers += [nn.Linear(sizes[j], sizes[j + 1], act())]\n",
        "  return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Make the environment\n",
        "2. Define the policy for the environment\n",
        "3. Compute actions via the policy\n",
        "4. Compute a loss via the actions"
      ],
      "metadata": {
        "id": "qeMJv3r3JZmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_name = 'CartPole-v0', hidden_sizes = [32], lr = 1e-2,\n",
        "          epochs = 50, batch_size = 5000, render = False):\n",
        "\n",
        "  # make environment\n",
        "  env = gym.make(env_name)\n",
        "  assert isinstance(env.observation_space, Box), \\\n",
        "    \"Error: This example only works for envs with continuous state spaces.\"\n",
        "  assert isinstance(env.action_space, Discrete), \\\n",
        "    \"Error: this example only works for environments with discrete action spaces\"\n",
        "\n",
        "  obs_dim = env.observation_space.shape[0]\n",
        "  act_dim = env.action_space.n\n",
        "\n",
        "  logits_net = mlp(sizes = [obs_dim] + hidden_sizes + [act_dim])\n",
        "\n",
        "  # get the policy - action distribution for observations of our states\n",
        "  def get_policy(obs):\n",
        "    logits = logits_net(obs)\n",
        "    return Categorical(logits = logits)\n",
        "\n",
        "  # assumes we make 1 set of observations. computes the action to take according to the policy\n",
        "  def get_action(obs):\n",
        "    return get_policy(obs).sample().item()\n",
        "\n",
        "  def compute_loss(obs, act, weights):\n",
        "    logp = get_policy(obs).log_prob(act)\n",
        "    return -(logp * weights).mean()\n",
        "\n",
        "  optimizer = Adam(logits_net.parameters(), lr = lr)\n",
        "\n",
        "  # for training the policy\n",
        "  def train_one_epoch():\n",
        "    batch_obs = []      # observatoins\n",
        "    batch_acts = []     # actions\n",
        "    batch_weights = []  # weights for our trajectory returns\n",
        "    batch_rets = []     # for trajectory returns\n",
        "    batch_lens = []     # for trajectory lengths\n",
        "\n",
        "    obs = env.reset() # first obs from starting dist\n",
        "    done = False      # are we done or not\n",
        "    ep_rews = []      # rewards acquired throughout episode\n",
        "\n",
        "    finished_rendering_epoch = False # what does this do\n",
        "\n",
        "    while True:\n",
        "\n",
        "      # render\n",
        "      if (not finished_rendering_epoch) and render:\n",
        "        env.render()\n",
        "\n",
        "      # save current observation\n",
        "      batch_obs.append(obs.copy())\n",
        "\n",
        "      # act\n",
        "      act = get_action(torch.as_tensor(obs, dtype = torch.float32))\n",
        "      obs, rew, done, info = env.step(act)\n",
        "\n",
        "      # save action & reward\n",
        "      batch_acts.append(act)\n",
        "      ep_rews.append(rew)\n",
        "\n",
        "      if done:\n",
        "\n",
        "        ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "        batch_rets.append(ep_ret)\n",
        "        batch_lens.append(ep_len)\n",
        "\n",
        "        # weight for each logprob(a|s) is the return for the trajectory\n",
        "        batch_weights += [ep_ret] *ep_len\n",
        "\n",
        "        # reset values\n",
        "        obs, done, ep_rews = env.reset(), False, []\n",
        "\n",
        "        # don't render epoch again, i.e only render an epoch for 1 trajectory\n",
        "        finished_rendering_epoch = True\n",
        "\n",
        "        # break if we have all our episodes for this epoch\n",
        "        if len(batch_obs) > batch_size:\n",
        "          break\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss = compute_loss(obs = torch.as_tensor(batch_obs, dtype = torch.float32),\n",
        "                              act = torch.as_tensor(batch_acts, dtype = torch.float32),\n",
        "                              weights = torch.as_tensor(batch_weights, dtype = torch.float32))\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "  for i in range(epochs):\n",
        "    batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "    print(f'epoch: {i:3d} \\t loss: {batch_loss:.3f} \\t return: {np.mean(batch_rets):.3f} \\t ep_len: {np.mean(batch_lens):.3f}')"
      ],
      "metadata": {
        "id": "O9Ngvk7LDhsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(env_name = 'CartPole-v1', hidden_sizes = [32], lr = 1e-2,\n",
        "          epochs = 50, batch_size = 5000, render = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ0cITVTWxgj",
        "outputId": "9edbc625-0ad4-4c08-e8fa-ad1abf3178f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-21-1512d1cd188d>:82: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  batch_loss = compute_loss(obs = torch.as_tensor(batch_obs, dtype = torch.float32),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:   0 \t loss: 18.276 \t return: 20.727 \t ep_len: 20.727\n",
            "epoch:   1 \t loss: 20.534 \t return: 22.786 \t ep_len: 22.786\n",
            "epoch:   2 \t loss: 23.125 \t return: 26.120 \t ep_len: 26.120\n",
            "epoch:   3 \t loss: 24.620 \t return: 27.522 \t ep_len: 27.522\n",
            "epoch:   4 \t loss: 28.486 \t return: 32.348 \t ep_len: 32.348\n",
            "epoch:   5 \t loss: 31.794 \t return: 37.154 \t ep_len: 37.154\n",
            "epoch:   6 \t loss: 33.761 \t return: 40.508 \t ep_len: 40.508\n",
            "epoch:   7 \t loss: 35.423 \t return: 42.227 \t ep_len: 42.227\n",
            "epoch:   8 \t loss: 39.483 \t return: 46.907 \t ep_len: 46.907\n",
            "epoch:   9 \t loss: 32.011 \t return: 42.441 \t ep_len: 42.441\n",
            "epoch:  10 \t loss: 40.096 \t return: 50.717 \t ep_len: 50.717\n",
            "epoch:  11 \t loss: 37.258 \t return: 48.553 \t ep_len: 48.553\n",
            "epoch:  12 \t loss: 51.118 \t return: 66.461 \t ep_len: 66.461\n",
            "epoch:  13 \t loss: 47.593 \t return: 62.825 \t ep_len: 62.825\n",
            "epoch:  14 \t loss: 54.957 \t return: 70.465 \t ep_len: 70.465\n",
            "epoch:  15 \t loss: 55.876 \t return: 79.672 \t ep_len: 79.672\n",
            "epoch:  16 \t loss: 53.468 \t return: 72.623 \t ep_len: 72.623\n",
            "epoch:  17 \t loss: 63.569 \t return: 80.710 \t ep_len: 80.710\n",
            "epoch:  18 \t loss: 58.933 \t return: 75.731 \t ep_len: 75.731\n",
            "epoch:  19 \t loss: 74.347 \t return: 110.348 \t ep_len: 110.348\n",
            "epoch:  20 \t loss: 70.752 \t return: 100.700 \t ep_len: 100.700\n",
            "epoch:  21 \t loss: 74.917 \t return: 108.062 \t ep_len: 108.062\n",
            "epoch:  22 \t loss: 67.340 \t return: 98.922 \t ep_len: 98.922\n",
            "epoch:  23 \t loss: 72.416 \t return: 105.292 \t ep_len: 105.292\n",
            "epoch:  24 \t loss: 70.718 \t return: 105.396 \t ep_len: 105.396\n",
            "epoch:  25 \t loss: 77.388 \t return: 118.651 \t ep_len: 118.651\n",
            "epoch:  26 \t loss: 97.362 \t return: 154.152 \t ep_len: 154.152\n",
            "epoch:  27 \t loss: 95.012 \t return: 151.882 \t ep_len: 151.882\n",
            "epoch:  28 \t loss: 94.494 \t return: 154.970 \t ep_len: 154.970\n",
            "epoch:  29 \t loss: 98.524 \t return: 163.806 \t ep_len: 163.806\n",
            "epoch:  30 \t loss: 93.954 \t return: 162.710 \t ep_len: 162.710\n",
            "epoch:  31 \t loss: 85.626 \t return: 148.882 \t ep_len: 148.882\n",
            "epoch:  32 \t loss: 83.334 \t return: 151.029 \t ep_len: 151.029\n",
            "epoch:  33 \t loss: 102.189 \t return: 168.233 \t ep_len: 168.233\n",
            "epoch:  34 \t loss: 87.911 \t return: 145.314 \t ep_len: 145.314\n",
            "epoch:  35 \t loss: 88.644 \t return: 141.667 \t ep_len: 141.667\n",
            "epoch:  36 \t loss: 105.553 \t return: 173.655 \t ep_len: 173.655\n",
            "epoch:  37 \t loss: 95.527 \t return: 157.125 \t ep_len: 157.125\n",
            "epoch:  38 \t loss: 95.564 \t return: 162.871 \t ep_len: 162.871\n",
            "epoch:  39 \t loss: 89.172 \t return: 159.094 \t ep_len: 159.094\n",
            "epoch:  40 \t loss: 112.651 \t return: 175.276 \t ep_len: 175.276\n",
            "epoch:  41 \t loss: 98.157 \t return: 166.867 \t ep_len: 166.867\n",
            "epoch:  42 \t loss: 105.492 \t return: 178.207 \t ep_len: 178.207\n",
            "epoch:  43 \t loss: 97.094 \t return: 168.567 \t ep_len: 168.567\n",
            "epoch:  44 \t loss: 113.289 \t return: 180.714 \t ep_len: 180.714\n",
            "epoch:  45 \t loss: 104.345 \t return: 180.536 \t ep_len: 180.536\n",
            "epoch:  46 \t loss: 122.121 \t return: 206.920 \t ep_len: 206.920\n",
            "epoch:  47 \t loss: 134.101 \t return: 220.913 \t ep_len: 220.913\n",
            "epoch:  48 \t loss: 138.026 \t return: 220.435 \t ep_len: 220.435\n",
            "epoch:  49 \t loss: 142.105 \t return: 238.455 \t ep_len: 238.455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcEmLALWW4z7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}